{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precursors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, you first need to convert your sequences and targets into the input HDF5 format. Check out my tutorials for how to do that; they're linked from the [main page](../README.md).\n",
    "\n",
    "For this tutorial, grab a small example HDF5 that I constructed here with 10% of the training sequences and only GM12878 targets for various DNase-seq, ChIP-seq, and CAGE experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/heart_l131k.h5'):\n",
    "    subprocess.call('curl -o data/heart_l131k.h5 https://storage.googleapis.com/basenji_tutorial_data/heart_l131k.h5', shell=True)\n",
    "    subprocess.call('curl -o data/heart_l131k.bed https://storage.googleapis.com/basenji_tutorial_data/heart_l131k.bed', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to decide what sort of architecture to use. This grammar probably needs work; my goal was to enable hyperparameter searches to write the parameters to file so that I could run parallel training jobs to explore the hyperparameter space. I included an example set of parameters that will work well with this data in models/params_small.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run [basenji_train.py](https://github.com/calico/basenji/blob/master/bin/basenji_train.py) to train a model. The program will offer training feedback via stdout and write the model output files to the prefix given by the *-s* parameter.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --augment_rc | True | Process even-numbered epochs as forward, odd-numbered as reverse complemented. |\n",
    "| --ensemble_rc | True | Average forward and reverse complemented predictions on validation set. |\n",
    "| --augment_shifts | \"1,0,-1\" | Rotate epochs over small sequence shifts. |\n",
    "| --logdir | models/heart | Directory to save training logs and model checkpoints. |\n",
    "| --params | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| --data | data/heart_l131k.h5 | HDF5 file containing the training and validation input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train, uncomment the following line and run it. Depending on your hardware, it may require several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 4, 'batch_buffer': 4096, 'link': 'softplus', 'loss': 'poisson', 'optimizer': 'adam', 'adam_beta1': 0.97, 'adam_beta2': 0.98, 'learning_rate': 0.002, 'num_targets': 3, 'target_pool': 128, 'seq_length': 131072, 'target_length': 1024, 'cnn_dropout': 0.1, 'cnn_filter_sizes': [20, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 1], 'cnn_filters': [128, 128, 192, 256, 256, 32, 32, 32, 32, 32, 32, 384], 'cnn_pool': [2, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0], 'cnn_dilation': [1, 1, 1, 1, 1, 2, 4, 8, 16, 32, 64, 1], 'cnn_dense': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]}\n",
      "Cannot order TFRecords data/heart_l131k/tfrecords/train-0.tfr\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/tfrecord_batcher.py:91: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "Cannot order TFRecords data/heart_l131k/tfrecords/valid-0.tfr\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Targets pooled by 128 to length 1024\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:53: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:74: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:82: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:103: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/seqnn.py:295: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Convolution w/ 3 384x1 filters to final targets\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Targets pooled by 128 to length 1024\n",
      "Convolution w/ 3 384x1 filters to final targets\n",
      "2019-03-21 15:23:44.712773: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-21 15:23:44.747349: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\n",
      "2019-03-21 15:23:44.749960: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x558f7cad0900 executing computations on platform Host. Devices:\n",
      "2019-03-21 15:23:44.750021: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-03-21 15:23:44.879872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-21 15:23:44.880292: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x558f7d630a30 executing computations on platform CUDA. Devices:\n",
      "2019-03-21 15:23:44.880306: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\n",
      "2019-03-21 15:23:44.880468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7715\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 7.33GiB\n",
      "2019-03-21 15:23:44.880478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-03-21 15:23:44.881677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-21 15:23:44.881687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-03-21 15:23:44.881691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-03-21 15:23:44.881803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7127 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/bin/basenji_train.py:79: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
      "Initializing...\n",
      "Initialization time 1.785105\n",
      "2019-03-21 15:23:48.188043: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "Epoch:   1,  Steps:      63,  Train loss: 0.52385,  Valid loss: 0.49726,  Valid R2: 0.10814,  Valid R: 0.36375, Time:  26s, best!\n",
      "Epoch:   2,  Steps:     127,  Train loss: 0.46485,  Valid loss: 0.46488,  Valid R2: 0.14308,  Valid R: 0.46826, Time:  23s, best!\n",
      "Epoch:   3,  Steps:     191,  Train loss: 0.45367,  Valid loss: 0.63869,  Valid R2: 0.03335,  Valid R: 0.40897, Time:  23s\n",
      "Epoch:   4,  Steps:     255,  Train loss: 0.44842,  Valid loss: 0.45344,  Valid R2: 0.17916,  Valid R: 0.43719, Time:  23s, best!\n",
      "Epoch:   5,  Steps:     319,  Train loss: 0.44287,  Valid loss: 0.46815,  Valid R2: 0.21127,  Valid R: 0.47406, Time:  22s\n",
      "Epoch:   6,  Steps:     383,  Train loss: 0.43283,  Valid loss: 0.43528,  Valid R2: 0.27246,  Valid R: 0.52630, Time:  23s, best!\n",
      "Epoch:   7,  Steps:     447,  Train loss: 0.43148,  Valid loss: 0.45672,  Valid R2: 0.26808,  Valid R: 0.54762, Time:  22s\n",
      "Epoch:   8,  Steps:     511,  Train loss: 0.42253,  Valid loss: 0.43803,  Valid R2: 0.30155,  Valid R: 0.57488, Time:  22s\n",
      "Epoch:   9,  Steps:     575,  Train loss: 0.42117,  Valid loss: 0.44889,  Valid R2: 0.26345,  Valid R: 0.62513, Time:  22s\n",
      "Epoch:  10,  Steps:     639,  Train loss: 0.41695,  Valid loss: 0.42796,  Valid R2: 0.36358,  Valid R: 0.65998, Time:  23s, best!\n",
      "Epoch:  11,  Steps:     703,  Train loss: 0.41351,  Valid loss: 0.52076,  Valid R2: 0.19960,  Valid R: 0.56095, Time:  23s\n",
      "Epoch:  12,  Steps:     767,  Train loss: 0.41053,  Valid loss: 0.48110,  Valid R2: 0.30603,  Valid R: 0.68946, Time:  22s\n",
      "Epoch:  13,  Steps:     831,  Train loss: 0.40594,  Valid loss: 1.09352,  Valid R2: -0.64521,  Valid R: 0.50244, Time:  22s\n",
      "Epoch:  14,  Steps:     895,  Train loss: 0.40075,  Valid loss: 0.42558,  Valid R2: 0.46750,  Valid R: 0.73949, Time:  23s, best!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15,  Steps:     959,  Train loss: 0.39887,  Valid loss: 0.40544,  Valid R2: 0.46039,  Valid R: 0.78452, Time:  23s, best!\n",
      "Epoch:  16,  Steps:    1023,  Train loss: 0.39231,  Valid loss: 0.40487,  Valid R2: 0.47655,  Valid R: 0.77677, Time:  23s, best!\n",
      "Epoch:  17,  Steps:    1087,  Train loss: 0.39062,  Valid loss: 0.40256,  Valid R2: 0.44888,  Valid R: 0.80568, Time:  23s, best!\n",
      "Epoch:  18,  Steps:    1151,  Train loss: 0.38852,  Valid loss: 0.43482,  Valid R2: 0.44275,  Valid R: 0.77536, Time:  22s\n",
      "Epoch:  19,  Steps:    1215,  Train loss: 0.38776,  Valid loss: 0.40266,  Valid R2: 0.42566,  Valid R: 0.81120, Time:  22s\n",
      "Epoch:  20,  Steps:    1279,  Train loss: 0.38313,  Valid loss: 0.37566,  Valid R2: 0.61317,  Valid R: 0.83522, Time:  23s, best!\n",
      "Epoch:  21,  Steps:    1343,  Train loss: 0.38377,  Valid loss: 0.39357,  Valid R2: 0.58004,  Valid R: 0.82461, Time:  23s\n",
      "Epoch:  22,  Steps:    1407,  Train loss: 0.37974,  Valid loss: 0.37454,  Valid R2: 0.65444,  Valid R: 0.83325, Time:  23s, best!\n",
      "Epoch:  23,  Steps:    1471,  Train loss: 0.37851,  Valid loss: 0.39947,  Valid R2: 0.57129,  Valid R: 0.83719, Time:  23s\n",
      "Epoch:  24,  Steps:    1535,  Train loss: 0.37727,  Valid loss: 0.38190,  Valid R2: 0.58995,  Valid R: 0.81730, Time:  22s\n",
      "Epoch:  25,  Steps:    1599,  Train loss: 0.37329,  Valid loss: 0.41663,  Valid R2: 0.62261,  Valid R: 0.83653, Time:  22s\n",
      "Epoch:  26,  Steps:    1663,  Train loss: 0.37155,  Valid loss: 0.70980,  Valid R2: 0.31987,  Valid R: 0.78390, Time:  22s\n",
      "Epoch:  27,  Steps:    1727,  Train loss: 0.36838,  Valid loss: 0.38150,  Valid R2: 0.60313,  Valid R: 0.85557, Time:  22s\n",
      "Epoch:  28,  Steps:    1791,  Train loss: 0.36500,  Valid loss: 0.36801,  Valid R2: 0.64463,  Valid R: 0.85727, Time:  22s, best!\n",
      "Epoch:  29,  Steps:    1855,  Train loss: 0.36460,  Valid loss: 0.36107,  Valid R2: 0.68457,  Valid R: 0.86609, Time:  23s, best!\n",
      "Epoch:  30,  Steps:    1919,  Train loss: 0.36761,  Valid loss: 0.40011,  Valid R2: 0.54965,  Valid R: 0.84446, Time:  23s\n",
      "Epoch:  31,  Steps:    1983,  Train loss: 0.36234,  Valid loss: 0.37588,  Valid R2: 0.63883,  Valid R: 0.86350, Time:  23s\n",
      "Epoch:  32,  Steps:    2047,  Train loss: 0.36125,  Valid loss: 0.38905,  Valid R2: 0.70607,  Valid R: 0.85622, Time:  22s\n",
      "Epoch:  33,  Steps:    2111,  Train loss: 0.36132,  Valid loss: 0.36181,  Valid R2: 0.71800,  Valid R: 0.86002, Time:  23s\n",
      "Epoch:  34,  Steps:    2175,  Train loss: 0.36118,  Valid loss: 0.36171,  Valid R2: 0.72215,  Valid R: 0.86740, Time:  24s\n",
      "Epoch:  35,  Steps:    2239,  Train loss: 0.35832,  Valid loss: 0.35252,  Valid R2: 0.72497,  Valid R: 0.86676, Time:  23s, best!\n",
      "Epoch:  36,  Steps:    2303,  Train loss: 0.35762,  Valid loss: 0.37825,  Valid R2: 0.62635,  Valid R: 0.87072, Time:  23s\n",
      "Epoch:  37,  Steps:    2367,  Train loss: 0.35528,  Valid loss: 0.36686,  Valid R2: 0.70884,  Valid R: 0.88079, Time:  23s\n",
      "Epoch:  38,  Steps:    2431,  Train loss: 0.35350,  Valid loss: 0.36854,  Valid R2: 0.70328,  Valid R: 0.87536, Time:  23s\n",
      "Epoch:  39,  Steps:    2495,  Train loss: 0.35549,  Valid loss: 0.39578,  Valid R2: 0.71540,  Valid R: 0.86534, Time:  23s\n",
      "Epoch:  40,  Steps:    2559,  Train loss: 0.35478,  Valid loss: 0.35265,  Valid R2: 0.74434,  Valid R: 0.88047, Time:  23s\n",
      "Epoch:  41,  Steps:    2623,  Train loss: 0.35396,  Valid loss: 0.36235,  Valid R2: 0.72010,  Valid R: 0.87754, Time:  23s\n",
      "Epoch:  42,  Steps:    2687,  Train loss: 0.35099,  Valid loss: 0.35702,  Valid R2: 0.71954,  Valid R: 0.87423, Time:  23s\n",
      "Epoch:  43,  Steps:    2751,  Train loss: 0.35096,  Valid loss: 0.34612,  Valid R2: 0.75572,  Valid R: 0.88515, Time:  23s, best!\n",
      "Epoch:  44,  Steps:    2815,  Train loss: 0.34849,  Valid loss: 0.36540,  Valid R2: 0.74012,  Valid R: 0.88956, Time:  22s\n",
      "Epoch:  45,  Steps:    2879,  Train loss: 0.34733,  Valid loss: 0.34848,  Valid R2: 0.75223,  Valid R: 0.89507, Time:  22s\n",
      "Epoch:  46,  Steps:    2943,  Train loss: 0.34751,  Valid loss: 0.36866,  Valid R2: 0.64134,  Valid R: 0.87818, Time:  22s\n",
      "Epoch:  47,  Steps:    3007,  Train loss: 0.34649,  Valid loss: 0.48290,  Valid R2: 0.68245,  Valid R: 0.87478, Time:  23s\n",
      "Epoch:  48,  Steps:    3071,  Train loss: 0.34557,  Valid loss: 0.37306,  Valid R2: 0.66589,  Valid R: 0.88758, Time:  23s\n",
      "Epoch:  49,  Steps:    3135,  Train loss: 0.34442,  Valid loss: 0.35683,  Valid R2: 0.74512,  Valid R: 0.89456, Time:  23s\n",
      "Epoch:  50,  Steps:    3199,  Train loss: 0.34368,  Valid loss: 0.34637,  Valid R2: 0.71026,  Valid R: 0.89393, Time:  22s\n",
      "Epoch:  51,  Steps:    3263,  Train loss: 0.34236,  Valid loss: 0.33381,  Valid R2: 0.80049,  Valid R: 0.90534, Time:  23s, best!\n",
      "Epoch:  52,  Steps:    3327,  Train loss: 0.34225,  Valid loss: 0.33929,  Valid R2: 0.76663,  Valid R: 0.89847, Time:  22s\n",
      "Epoch:  53,  Steps:    3391,  Train loss: 0.34086,  Valid loss: 0.35957,  Valid R2: 0.73565,  Valid R: 0.89437, Time:  22s\n",
      "Epoch:  54,  Steps:    3455,  Train loss: 0.33987,  Valid loss: 0.33808,  Valid R2: 0.75800,  Valid R: 0.90529, Time:  22s\n",
      "Epoch:  55,  Steps:    3519,  Train loss: 0.34039,  Valid loss: 0.35871,  Valid R2: 0.75567,  Valid R: 0.90082, Time:  22s\n",
      "Epoch:  56,  Steps:    3583,  Train loss: 0.33802,  Valid loss: 0.38475,  Valid R2: 0.73444,  Valid R: 0.89935, Time:  22s\n",
      "Epoch:  57,  Steps:    3647,  Train loss: 0.33727,  Valid loss: 0.34894,  Valid R2: 0.77496,  Valid R: 0.90594, Time:  22s\n",
      "Epoch:  58,  Steps:    3711,  Train loss: 0.33669,  Valid loss: 0.34294,  Valid R2: 0.77220,  Valid R: 0.90459, Time:  22s\n",
      "Epoch:  59,  Steps:    3775,  Train loss: 0.33644,  Valid loss: 0.32786,  Valid R2: 0.79550,  Valid R: 0.91175, Time:  23s, best!\n",
      "Epoch:  60,  Steps:    3839,  Train loss: 0.33541,  Valid loss: 0.33026,  Valid R2: 0.80678,  Valid R: 0.90775, Time:  22s\n",
      "Epoch:  61,  Steps:    3903,  Train loss: 0.33439,  Valid loss: 0.34126,  Valid R2: 0.78273,  Valid R: 0.90621, Time:  22s\n",
      "Epoch:  62,  Steps:    3967,  Train loss: 0.33329,  Valid loss: 0.32852,  Valid R2: 0.80075,  Valid R: 0.90264, Time:  22s\n",
      "Epoch:  63,  Steps:    4031,  Train loss: 0.33245,  Valid loss: 0.32564,  Valid R2: 0.80401,  Valid R: 0.91394, Time:  22s, best!\n",
      "Epoch:  64,  Steps:    4095,  Train loss: 0.33117,  Valid loss: 0.34224,  Valid R2: 0.78877,  Valid R: 0.90721, Time:  22s\n",
      "Epoch:  65,  Steps:    4159,  Train loss: 0.33211,  Valid loss: 0.33813,  Valid R2: 0.76566,  Valid R: 0.90965, Time:  22s\n",
      "Epoch:  66,  Steps:    4223,  Train loss: 0.33099,  Valid loss: 0.33295,  Valid R2: 0.78294,  Valid R: 0.91529, Time:  22s\n",
      "Epoch:  67,  Steps:    4287,  Train loss: 0.32941,  Valid loss: 0.32998,  Valid R2: 0.78974,  Valid R: 0.91754, Time:  23s\n",
      "Epoch:  68,  Steps:    4351,  Train loss: 0.32839,  Valid loss: 0.33586,  Valid R2: 0.78627,  Valid R: 0.91069, Time:  23s\n",
      "Epoch:  69,  Steps:    4415,  Train loss: 0.32779,  Valid loss: 0.33267,  Valid R2: 0.75790,  Valid R: 0.91386, Time:  22s\n",
      "Epoch:  70,  Steps:    4479,  Train loss: 0.32700,  Valid loss: 0.31937,  Valid R2: 0.79985,  Valid R: 0.92407, Time:  23s, best!\n",
      "Epoch:  71,  Steps:    4543,  Train loss: 0.32642,  Valid loss: 0.32361,  Valid R2: 0.81511,  Valid R: 0.92089, Time:  22s\n",
      "Epoch:  72,  Steps:    4607,  Train loss: 0.32799,  Valid loss: 0.32025,  Valid R2: 0.81880,  Valid R: 0.91629, Time:  23s\n",
      "Epoch:  73,  Steps:    4671,  Train loss: 0.32736,  Valid loss: 0.32507,  Valid R2: 0.79693,  Valid R: 0.91535, Time:  22s\n",
      "Epoch:  74,  Steps:    4735,  Train loss: 0.32498,  Valid loss: 0.32165,  Valid R2: 0.80528,  Valid R: 0.92419, Time:  22s\n",
      "Epoch:  75,  Steps:    4799,  Train loss: 0.32509,  Valid loss: 0.32062,  Valid R2: 0.83211,  Valid R: 0.92331, Time:  22s\n",
      "Epoch:  76,  Steps:    4863,  Train loss: 0.32461,  Valid loss: 0.32341,  Valid R2: 0.81138,  Valid R: 0.92302, Time:  22s\n",
      "Epoch:  77,  Steps:    4927,  Train loss: 0.32287,  Valid loss: 0.32234,  Valid R2: 0.77515,  Valid R: 0.92798, Time:  23s\n",
      "Epoch:  78,  Steps:    4991,  Train loss: 0.32296,  Valid loss: 0.33018,  Valid R2: 0.79287,  Valid R: 0.92407, Time:  23s\n",
      "Epoch:  79,  Steps:    5055,  Train loss: 0.32380,  Valid loss: 0.32828,  Valid R2: 0.80821,  Valid R: 0.92301, Time:  22s\n",
      "Epoch:  80,  Steps:    5119,  Train loss: 0.32074,  Valid loss: 0.32009,  Valid R2: 0.77485,  Valid R: 0.92839, Time:  22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  81,  Steps:    5183,  Train loss: 0.32200,  Valid loss: 0.31475,  Valid R2: 0.82767,  Valid R: 0.92970, Time:  22s, best!\n",
      "Epoch:  82,  Steps:    5247,  Train loss: 0.32117,  Valid loss: 0.34454,  Valid R2: 0.72293,  Valid R: 0.92221, Time:  23s\n",
      "Epoch:  83,  Steps:    5311,  Train loss: 0.31918,  Valid loss: 0.32885,  Valid R2: 0.80948,  Valid R: 0.93020, Time:  22s\n",
      "Epoch:  84,  Steps:    5375,  Train loss: 0.31956,  Valid loss: 0.32034,  Valid R2: 0.82598,  Valid R: 0.92904, Time:  22s\n",
      "Epoch:  85,  Steps:    5439,  Train loss: 0.31979,  Valid loss: 0.30866,  Valid R2: 0.81905,  Valid R: 0.93125, Time:  23s, best!\n",
      "Epoch:  86,  Steps:    5503,  Train loss: 0.31912,  Valid loss: 0.31603,  Valid R2: 0.83422,  Valid R: 0.93020, Time:  22s\n",
      "Epoch:  87,  Steps:    5567,  Train loss: 0.31698,  Valid loss: 0.32375,  Valid R2: 0.78543,  Valid R: 0.92687, Time:  22s\n",
      "Epoch:  88,  Steps:    5631,  Train loss: 0.31726,  Valid loss: 0.31518,  Valid R2: 0.82958,  Valid R: 0.92835, Time:  22s\n",
      "Epoch:  89,  Steps:    5695,  Train loss: 0.31637,  Valid loss: 0.33043,  Valid R2: 0.80622,  Valid R: 0.92523, Time:  22s\n",
      "Epoch:  90,  Steps:    5759,  Train loss: 0.31557,  Valid loss: 0.31987,  Valid R2: 0.82843,  Valid R: 0.92507, Time:  22s\n",
      "Epoch:  91,  Steps:    5823,  Train loss: 0.31728,  Valid loss: 0.31326,  Valid R2: 0.82723,  Valid R: 0.91504, Time:  22s\n",
      "Epoch:  92,  Steps:    5887,  Train loss: 0.31706,  Valid loss: 0.32107,  Valid R2: 0.80814,  Valid R: 0.92960, Time:  22s\n",
      "Epoch:  93,  Steps:    5951,  Train loss: 0.31447,  Valid loss: 0.30999,  Valid R2: 0.79222,  Valid R: 0.93203, Time:  22s\n",
      "Epoch:  94,  Steps:    6015,  Train loss: 0.31473,  Valid loss: 0.31669,  Valid R2: 0.78501,  Valid R: 0.93696, Time:  23s\n",
      "Epoch:  95,  Steps:    6079,  Train loss: 0.31438,  Valid loss: 0.31028,  Valid R2: 0.83973,  Valid R: 0.92923, Time:  23s\n",
      "Epoch:  96,  Steps:    6143,  Train loss: 0.31392,  Valid loss: 0.31033,  Valid R2: 0.82817,  Valid R: 0.93373, Time:  23s\n",
      "Epoch:  97,  Steps:    6207,  Train loss: 0.31293,  Valid loss: 0.31149,  Valid R2: 0.84788,  Valid R: 0.93761, Time:  23s\n",
      "Epoch:  98,  Steps:    6271,  Train loss: 0.31204,  Valid loss: 0.31564,  Valid R2: 0.82055,  Valid R: 0.93282, Time:  23s\n",
      "Epoch:  99,  Steps:    6335,  Train loss: 0.31152,  Valid loss: 0.30995,  Valid R2: 0.81150,  Valid R: 0.93649, Time:  22s\n",
      "Epoch: 100,  Steps:    6399,  Train loss: 0.31104,  Valid loss: 0.30823,  Valid R2: 0.82674,  Valid R: 0.93620, Time:  23s, best!\n",
      "Epoch: 101,  Steps:    6463,  Train loss: 0.31090,  Valid loss: 0.30815,  Valid R2: 0.82407,  Valid R: 0.93453, Time:  23s, best!\n",
      "Epoch: 102,  Steps:    6527,  Train loss: 0.31060,  Valid loss: 0.30320,  Valid R2: 0.83478,  Valid R: 0.94015, Time:  23s, best!\n",
      "Epoch: 103,  Steps:    6591,  Train loss: 0.31250,  Valid loss: 0.31047,  Valid R2: 0.84427,  Valid R: 0.93453, Time:  23s\n",
      "Epoch: 104,  Steps:    6655,  Train loss: 0.31007,  Valid loss: 0.30688,  Valid R2: 0.82315,  Valid R: 0.93092, Time:  22s\n",
      "Epoch: 105,  Steps:    6719,  Train loss: 0.31012,  Valid loss: 0.30959,  Valid R2: 0.83032,  Valid R: 0.93249, Time:  23s\n",
      "Epoch: 106,  Steps:    6783,  Train loss: 0.30794,  Valid loss: 0.30762,  Valid R2: 0.82905,  Valid R: 0.93868, Time:  22s\n",
      "Epoch: 107,  Steps:    6847,  Train loss: 0.30930,  Valid loss: 0.29776,  Valid R2: 0.82789,  Valid R: 0.94510, Time:  23s, best!\n",
      "Epoch: 108,  Steps:    6911,  Train loss: 0.30921,  Valid loss: 0.29881,  Valid R2: 0.86156,  Valid R: 0.93254, Time:  22s\n",
      "Epoch: 109,  Steps:    6975,  Train loss: 0.30837,  Valid loss: 0.29980,  Valid R2: 0.84709,  Valid R: 0.93911, Time:  23s\n",
      "Epoch: 110,  Steps:    7039,  Train loss: 0.30859,  Valid loss: 0.30618,  Valid R2: 0.85079,  Valid R: 0.93888, Time:  23s\n",
      "Epoch: 111,  Steps:    7103,  Train loss: 0.30738,  Valid loss: 0.32929,  Valid R2: 0.77242,  Valid R: 0.93265, Time:  22s\n",
      "Epoch: 112,  Steps:    7167,  Train loss: 0.30769,  Valid loss: 0.30313,  Valid R2: 0.82768,  Valid R: 0.94173, Time:  23s\n",
      "Epoch: 113,  Steps:    7231,  Train loss: 0.30593,  Valid loss: 0.32066,  Valid R2: 0.77489,  Valid R: 0.93544, Time:  23s\n",
      "Epoch: 114,  Steps:    7295,  Train loss: 0.30704,  Valid loss: 0.29287,  Valid R2: 0.86659,  Valid R: 0.94467, Time:  23s, best!\n",
      "Epoch: 115,  Steps:    7359,  Train loss: 0.30589,  Valid loss: 0.30143,  Valid R2: 0.83925,  Valid R: 0.94106, Time:  23s\n",
      "Epoch: 116,  Steps:    7423,  Train loss: 0.30549,  Valid loss: 0.30281,  Valid R2: 0.83736,  Valid R: 0.94264, Time:  23s\n",
      "Epoch: 117,  Steps:    7487,  Train loss: 0.30543,  Valid loss: 0.29139,  Valid R2: 0.86973,  Valid R: 0.94198, Time:  23s, best!\n",
      "Epoch: 118,  Steps:    7551,  Train loss: 0.30421,  Valid loss: 0.31070,  Valid R2: 0.80590,  Valid R: 0.93700, Time:  23s\n",
      "Epoch: 119,  Steps:    7615,  Train loss: 0.30485,  Valid loss: 0.29442,  Valid R2: 0.83823,  Valid R: 0.94495, Time:  23s\n",
      "Epoch: 120,  Steps:    7679,  Train loss: 0.30494,  Valid loss: 0.29999,  Valid R2: 0.84263,  Valid R: 0.93886, Time:  23s\n",
      "Epoch: 121,  Steps:    7743,  Train loss: 0.30293,  Valid loss: 0.29590,  Valid R2: 0.87452,  Valid R: 0.94240, Time:  23s\n",
      "Epoch: 122,  Steps:    7807,  Train loss: 0.30453,  Valid loss: 0.31031,  Valid R2: 0.81779,  Valid R: 0.93589, Time:  23s\n",
      "Epoch: 123,  Steps:    7871,  Train loss: 0.30362,  Valid loss: 0.29547,  Valid R2: 0.87319,  Valid R: 0.94396, Time:  22s\n",
      "Epoch: 124,  Steps:    7935,  Train loss: 0.30238,  Valid loss: 0.29613,  Valid R2: 0.84827,  Valid R: 0.94681, Time:  23s\n",
      "Epoch: 125,  Steps:    7999,  Train loss: 0.30286,  Valid loss: 0.29211,  Valid R2: 0.86286,  Valid R: 0.94673, Time:  23s\n",
      "Epoch: 126,  Steps:    8063,  Train loss: 0.30176,  Valid loss: 0.29308,  Valid R2: 0.84714,  Valid R: 0.94167, Time:  22s\n",
      "Epoch: 127,  Steps:    8127,  Train loss: 0.30252,  Valid loss: 0.29592,  Valid R2: 0.84002,  Valid R: 0.94460, Time:  22s\n",
      "Epoch: 128,  Steps:    8191,  Train loss: 0.30165,  Valid loss: 0.29117,  Valid R2: 0.84411,  Valid R: 0.94729, Time:  23s, best!\n",
      "Epoch: 129,  Steps:    8255,  Train loss: 0.30040,  Valid loss: 0.29712,  Valid R2: 0.86722,  Valid R: 0.94900, Time:  22s\n",
      "Epoch: 130,  Steps:    8319,  Train loss: 0.30079,  Valid loss: 0.29444,  Valid R2: 0.83076,  Valid R: 0.95097, Time:  22s\n",
      "Epoch: 131,  Steps:    8383,  Train loss: 0.29980,  Valid loss: 0.29748,  Valid R2: 0.85977,  Valid R: 0.94567, Time:  22s\n",
      "Epoch: 132,  Steps:    8447,  Train loss: 0.29907,  Valid loss: 0.29120,  Valid R2: 0.86291,  Valid R: 0.95083, Time:  22s\n",
      "Epoch: 133,  Steps:    8511,  Train loss: 0.29891,  Valid loss: 0.29810,  Valid R2: 0.83626,  Valid R: 0.94503, Time:  22s\n",
      "Epoch: 134,  Steps:    8575,  Train loss: 0.29985,  Valid loss: 0.29491,  Valid R2: 0.86540,  Valid R: 0.94521, Time:  22s\n",
      "Epoch: 135,  Steps:    8639,  Train loss: 0.29999,  Valid loss: 0.29084,  Valid R2: 0.85606,  Valid R: 0.94875, Time:  23s, best!\n",
      "Epoch: 136,  Steps:    8703,  Train loss: 0.29887,  Valid loss: 0.28961,  Valid R2: 0.85388,  Valid R: 0.94965, Time:  23s, best!\n",
      "Epoch: 137,  Steps:    8767,  Train loss: 0.30025,  Valid loss: 0.28937,  Valid R2: 0.86548,  Valid R: 0.94761, Time:  23s, best!\n",
      "Epoch: 138,  Steps:    8831,  Train loss: 0.29857,  Valid loss: 0.30101,  Valid R2: 0.84833,  Valid R: 0.94051, Time:  22s\n",
      "Epoch: 139,  Steps:    8895,  Train loss: 0.29861,  Valid loss: 0.28260,  Valid R2: 0.88286,  Valid R: 0.94874, Time:  23s, best!\n",
      "Epoch: 140,  Steps:    8959,  Train loss: 0.29771,  Valid loss: 0.29149,  Valid R2: 0.86127,  Valid R: 0.94255, Time:  22s\n",
      "Epoch: 141,  Steps:    9023,  Train loss: 0.29757,  Valid loss: 0.29056,  Valid R2: 0.84400,  Valid R: 0.95082, Time:  23s\n",
      "Epoch: 142,  Steps:    9087,  Train loss: 0.29698,  Valid loss: 0.29842,  Valid R2: 0.84856,  Valid R: 0.94334, Time:  22s\n",
      "Epoch: 143,  Steps:    9151,  Train loss: 0.29647,  Valid loss: 0.30257,  Valid R2: 0.81291,  Valid R: 0.94373, Time:  21s\n",
      "Epoch: 144,  Steps:    9215,  Train loss: 0.29621,  Valid loss: 0.28712,  Valid R2: 0.85936,  Valid R: 0.94961, Time:  22s\n",
      "Epoch: 145,  Steps:    9279,  Train loss: 0.29525,  Valid loss: 0.29047,  Valid R2: 0.84097,  Valid R: 0.95062, Time:  22s\n",
      "Epoch: 146,  Steps:    9343,  Train loss: 0.29683,  Valid loss: 0.28994,  Valid R2: 0.84538,  Valid R: 0.94950, Time:  23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! basenji_train.py --augment_rc --ensemble_rc --augment_shifts \"1,0,-1\" --logdir models/heart --params models/params_small.txt --train_data data/heart_l131k/tfrecords/train*.tfr --test_data data/heart_l131k/tfrecords/valid*.tfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just download a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('models/heart'):\n",
    "    os.mkdir('models/heart')\n",
    "if not os.path.isfile('models/heart/model_best.tf.meta'):\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.index https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.index', shell=True)\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.meta https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.meta', shell=True)\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.data-00000-of-00001 https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.data-00000-of-00001', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models/heart/model_best.tf will now specify the name of your saved model to be provided to other programs.\n",
    "\n",
    "To further benchmark the accuracy (e.g. computing significant \"peak\" accuracy), use [basenji_test.py](https://github.com/calico/basenji/blob/master/bin/basenji_test.py).\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --rc | | Average the forward and reverse complement to form prediction. |\n",
    "| -o | output/heart_test | Output directory. |\n",
    "| --ai | 0,1,2 | Make accuracy scatter plots for targets 0, 1, and 2. |\n",
    "| --ti | 3,4,5 | Make BigWig tracks for targets 3, 4, and 5. |\n",
    "| -t | data/heart_l131k.bed | BED file describing sequence regions for BigWig track output. |\n",
    "| params_file | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| model_file | models/heart/model_best.tf | Trained saved model prefix. |\n",
    "| data_file | data/heart_l131k.h5 | HDF5 file containing the test input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 4, 'batch_buffer': 4096, 'link': 'softplus', 'loss': 'poisson', 'optimizer': 'adam', 'adam_beta1': 0.97, 'adam_beta2': 0.98, 'learning_rate': 0.002, 'num_targets': 3, 'target_pool': 128, 'seq_length': 131072, 'target_length': 1024, 'cnn_dropout': 0.1, 'cnn_filter_sizes': [20, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 1], 'cnn_filters': [128, 128, 192, 256, 256, 32, 32, 32, 32, 32, 32, 384], 'cnn_pool': [2, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0], 'cnn_dilation': [1, 1, 1, 1, 1, 2, 4, 8, 16, 32, 64, 1], 'cnn_dense': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]}\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Targets pooled by 128 to length 1024\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:53: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:74: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:82: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/layers.py:103: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n",
      "WARNING:tensorflow:From /home/chendi/projects/XDNN/2017_07_Basenji/basenji/basenji/seqnn.py:295: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Convolution w/ 3 384x1 filters to final targets\n",
      "WARNING:tensorflow:From /home/chendi/anaconda3/envs/basenji/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Targets pooled by 128 to length 1024\n",
      "Convolution w/ 3 384x1 filters to final targets\n",
      "2019-04-03 16:52:41.966373: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-04-03 16:52:41.987004: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\n",
      "2019-04-03 16:52:41.987872: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x562a2a4ae5b0 executing computations on platform Host. Devices:\n",
      "2019-04-03 16:52:41.987884: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-04-03 16:52:42.088974: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8512602112\n",
      "2019-04-03 16:52:42.089044: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\n",
      "Aborted (core dumped)\n"
     ]
    }
   ],
   "source": [
    "! basenji_test.py --ai 0,1,2 -o output/heart_test --rc --shifts \"1,0,-1\" models/params_small.txt models/heart/model_best.tf data/heart_l131k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/heart_test/acc.txt* is a table specifiying the loss function value, R2, R2 after log2, and Spearman correlation for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  0.32012  0.31707  0.57520  0.65397  aorta\r\n",
      "   1  0.19305  0.27314  0.53032  0.72304  artery\r\n",
      "   2  0.74131  0.24615  0.51209  0.58342  pulmonic_valve\r\n"
     ]
    }
   ],
   "source": [
    "! cat output/heart_test/acc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories *pr*, *roc*, *violin*, and *scatter* in *data/heart_test* contain plots for the targets indexed by 0, 1, and 2 as specified by the --ai option above.\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"output/heart_test/pr/t0.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f20c4241320>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('output/heart_test/pr/t0.pdf', width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
